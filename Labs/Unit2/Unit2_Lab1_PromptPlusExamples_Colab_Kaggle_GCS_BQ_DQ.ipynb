{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JExLT-wXofe"
      },
      "source": [
        "# MGMT 467 — Prompt-Driven Lab (with Commented Examples)\n",
        "## Kaggle ➜ Google Cloud Storage ➜ BigQuery ➜ Data Quality (DQ)\n",
        "\n",
        "**How to use this notebook**\n",
        "- Each section gives you a **Build Prompt** to paste into Gemini/Vertex AI (or Gemini in Colab).\n",
        "- Below each prompt, you’ll see a **commented example** of what a good LLM answer might look like.\n",
        "- **Do not** just uncomment and run. Use the prompt to generate your own code, then compare to the example.\n",
        "- After every step, run the **Verification Prompt**, and write the **Reflection** in Markdown.\n",
        "\n",
        "> Goal today: Download the Netflix dataset (Kaggle) → Stage on GCS → Load into BigQuery → Run DQ profiling (missingness, duplicates, outliers, anomaly flags).\n"
      ],
      "id": "7JExLT-wXofe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l19Spz1_Xofh"
      },
      "source": [
        "### Academic integrity & LLM usage\n",
        "- Use the prompts here to generate your own code cells.\n",
        "- Read concept notes and write the reflection answers in your own words.\n",
        "- Keep credentials out of code. Upload `kaggle.json` when asked.\n"
      ],
      "id": "l19Spz1_Xofh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hQIbLhHXofi"
      },
      "source": [
        "## Learning objectives\n",
        "1) Explain **why** we stage data in GCS and load it to BigQuery.  \n",
        "2) Build an **idempotent**, auditable pipeline.  \n",
        "3) Diagnose **missingness**, **duplicates**, and **outliers** and justify cleaning choices.  \n",
        "4) Connect DQ decisions to **business/ML impact**.\n"
      ],
      "id": "4hQIbLhHXofi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgoJ9cMFXofi"
      },
      "source": [
        "## 0) Environment setup — What & Why\n",
        "Authenticate Colab to Google Cloud so we can use `gcloud`, GCS, and BigQuery. Set **PROJECT_ID** and **REGION** once for consistency (cost/latency)."
      ],
      "id": "JgoJ9cMFXofi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTre2VfPXofi"
      },
      "source": [
        "### Build Prompt (paste to LLM)\n",
        "You are my cloud TA. Generate a single **Colab code cell** that:\n",
        "1) Authenticates to Google Cloud in Colab,  \n",
        "2) Prompts for `PROJECT_ID` via `input()` and sets `REGION=\"us-central1\"` (editable),  \n",
        "3) Exports `GOOGLE_CLOUD_PROJECT`,  \n",
        "4) Runs `gcloud config set project $GOOGLE_CLOUD_PROJECT`,  \n",
        "5) Prints both values. Add 2–3 comments explaining what/why.\n",
        "End with a comment: `# Done: Auth + Project/Region set`.\n"
      ],
      "id": "wTre2VfPXofi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21711ca2",
        "outputId": "2d21877c-b565-4e8d-b38b-b8943b331c1e"
      },
      "source": [
        "# Authenticate to Google Cloud\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "# Prompt for project ID and set region\n",
        "PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "REGION = \"us-central1\"  # default region\n",
        "\n",
        "# Export project ID as environment variable for gcloud\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "os.environ[\"REGION\"] = REGION\n",
        "print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)\n",
        "\n",
        "# Set active project for gcloud/BigQuery CLI\n",
        "# This ensures subsequent gcloud commands use the correct project\n",
        "!gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "# Verify the project is set\n",
        "!gcloud config get-value project\n",
        "\n",
        "# Done: Auth + Project/Region set"
      ],
      "id": "21711ca2",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GCP Project ID: mgmt467-472519\n",
            "Project: mgmt467-472519 | Region: us-central1\n",
            "Updated property [core/project].\n",
            "mgmt467-472519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2eTNL3hXofk"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a short cell that prints the active project using `gcloud config get-value project` and echoes the `REGION` you set.\n"
      ],
      "id": "F2eTNL3hXofk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c90ba9e7",
        "outputId": "5f8baed6-6609-4946-9539-53f92b35f3ea"
      },
      "source": [
        "# Verify the active project\n",
        "!gcloud config get-value project\n",
        "\n",
        "# Echo the set region\n",
        "import os\n",
        "print(\"REGION:\", os.environ.get(\"REGION\", \"REGION not set\"))"
      ],
      "id": "c90ba9e7",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mgmt467-472519\n",
            "REGION: us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVU4NfhbXofk"
      },
      "source": [
        "**Reflection:** Why do we set `PROJECT_ID` and `REGION` at the top? What can go wrong if we don’t?"
      ],
      "id": "MVU4NfhbXofk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Region and Project_ID are set at the top to prevent errors later in the process if we forget to set."
      ],
      "metadata": {
        "id": "ffRhKkJ6c4Wo"
      },
      "id": "ffRhKkJ6c4Wo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLZGe0s4Xofk"
      },
      "source": [
        "## 1) Kaggle API — What & Why\n",
        "Use Kaggle CLI for reproducible downloads. Store `kaggle.json` at `~/.kaggle/kaggle.json` with `0600` permissions to protect secrets."
      ],
      "id": "nLZGe0s4Xofk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52yoxmj3Xofl"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **single Colab code cell** that:\n",
        "- Prompts me to upload `kaggle.json`,\n",
        "- Saves to `~/.kaggle/kaggle.json` with `0600` permissions,\n",
        "- Prints `kaggle --version`.\n",
        "Add comments about security and reproducibility.\n"
      ],
      "id": "52yoxmj3Xofl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "efb615ad",
        "outputId": "8b64aa0f-3aa4-4af0-d30c-9ae60e906641"
      },
      "source": [
        "# Prompt the user to upload their kaggle.json file\n",
        "# This file contains your Kaggle API credentials.\n",
        "# It's important to keep this file secure.\n",
        "from google.colab import files\n",
        "print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Save the uploaded file to the correct directory and set permissions\n",
        "# This ensures the Kaggle CLI can find your credentials.\n",
        "# Setting permissions to 0600 makes the file readable/writable only by the owner,\n",
        "# which is crucial for security and reproducibility.\n",
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "    f.write(uploaded[list(uploaded.keys())[0]])\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)  # owner-only read/write permissions\n",
        "\n",
        "# Verify the Kaggle CLI is installed and configured correctly\n",
        "# This helps ensure the next steps involving the Kaggle API will work.\n",
        "!kaggle --version"
      ],
      "id": "efb615ad",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your kaggle.json (Kaggle > Account > Create New API Token)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4f876fd3-cf93-481e-9095-e17d2e867fca\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4f876fd3-cf93-481e-9095-e17d2e867fca\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZV_K9xwXofl"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a one-liner that runs `kaggle --help | head -n 20` to show the CLI is ready.\n"
      ],
      "id": "8ZV_K9xwXofl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3de8f29",
        "outputId": "b3683311-9ff1-44f4-80e2-72566846f46d"
      },
      "source": [
        "# Verify Kaggle CLI is ready\n",
        "!kaggle --help | head -n 20"
      ],
      "id": "d3de8f29",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: kaggle [-h] [-v] [-W]\n",
            "              {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "              ...\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -v, --version         Print the Kaggle API version\n",
            "  -W, --no-warn         Disable out-of-date API version warning\n",
            "\n",
            "commands:\n",
            "  {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "                        Use one of:\n",
            "                        competitions {list, files, download, submit, submissions, leaderboard}\n",
            "                        datasets {list, files, download, create, version, init, metadata, status}\n",
            "                        kernels {list, files, init, push, pull, output, status}\n",
            "                        models {instances, get, list, init, create, delete, update}\n",
            "                        models instances {versions, get, files, init, create, delete, update}\n",
            "                        models instances versions {init, create, download, delete, files}\n",
            "                        config {view, set, unset}\n",
            "    competitions (c)    Commands related to Kaggle competitions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQaiw6CIXofl"
      },
      "source": [
        "**Reflection:** Why require strict `0600` permissions on API tokens? What risks are we avoiding?"
      ],
      "id": "VQaiw6CIXofl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f64G_f9XXofm"
      },
      "source": [
        "## 2) Download & unzip dataset — What & Why\n",
        "Keep raw files under `/content/data/raw` for predictable paths and auditing.\n",
        "**Dataset:** `sayeeduddin/netflix-2025user-behavior-dataset-210k-records`"
      ],
      "id": "f64G_f9XXofm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ3JgUuvXofm"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates `/content/data/raw`,\n",
        "- Downloads the dataset to `/content/data` with Kaggle CLI,\n",
        "- Unzips into `/content/data/raw` (overwrite OK),\n",
        "- Lists all CSVs with sizes in a neat table.\n",
        "Include comments describing each step.\n"
      ],
      "id": "NZ3JgUuvXofm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f100d5af",
        "outputId": "02e3bb66-daa8-4aad-cdbb-c03d2a21f0f1"
      },
      "source": [
        "# Create a directory to store the raw data\n",
        "# This ensures a consistent location for downloaded files.\n",
        "!mkdir -p /content/data/raw\n",
        "\n",
        "# Download the dataset using the Kaggle CLI\n",
        "# The dataset is downloaded to the /content/data directory.\n",
        "!kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "\n",
        "# Unzip the downloaded dataset into the raw data directory\n",
        "# The -o flag ensures existing files are overwritten if present.\n",
        "!unzip -o /content/data/*.zip -d /content/data/raw\n",
        "\n",
        "# List all CSV files in the raw data directory with their sizes\n",
        "# This provides a quick inventory of the downloaded files.\n",
        "!ls -lh /content/data/raw/*.csv"
      ],
      "id": "f100d5af",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sayeeduddin/netflix-2025user-behavior-dataset-210k-records\n",
            "License(s): CC0-1.0\n",
            "Downloading netflix-2025user-behavior-dataset-210k-records.zip to /content/data\n",
            "  0% 0.00/4.02M [00:00<?, ?B/s]\n",
            "100% 4.02M/4.02M [00:00<00:00, 1.03GB/s]\n",
            "Archive:  /content/data/netflix-2025user-behavior-dataset-210k-records.zip\n",
            "  inflating: /content/data/raw/README.md  \n",
            "  inflating: /content/data/raw/movies.csv  \n",
            "  inflating: /content/data/raw/recommendation_logs.csv  \n",
            "  inflating: /content/data/raw/reviews.csv  \n",
            "  inflating: /content/data/raw/search_logs.csv  \n",
            "  inflating: /content/data/raw/users.csv  \n",
            "  inflating: /content/data/raw/watch_history.csv  \n",
            "-rw-r--r-- 1 root root 114K Aug  2 19:36 /content/data/raw/movies.csv\n",
            "-rw-r--r-- 1 root root 4.5M Aug  2 19:36 /content/data/raw/recommendation_logs.csv\n",
            "-rw-r--r-- 1 root root 1.8M Aug  2 19:36 /content/data/raw/reviews.csv\n",
            "-rw-r--r-- 1 root root 2.2M Aug  2 19:36 /content/data/raw/search_logs.csv\n",
            "-rw-r--r-- 1 root root 1.6M Aug  2 19:36 /content/data/raw/users.csv\n",
            "-rw-r--r-- 1 root root 8.9M Aug  2 19:36 /content/data/raw/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxe55_svXofm"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that asserts there are exactly **six** CSV files and prints their names.\n"
      ],
      "id": "gxe55_svXofm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73de8fd7",
        "outputId": "82563c41-1be3-404c-b5cd-6ef444eb07b9"
      },
      "source": [
        "# Verify there are exactly six CSV files and print their names\n",
        "import glob\n",
        "csv_files = glob.glob('/content/data/raw/*.csv')\n",
        "print(\"Found\", len(csv_files), \"CSV files:\")\n",
        "for f in sorted(csv_files):\n",
        "    print(f)\n",
        "assert len(csv_files) == 6, f\"Expected 6 CSV files, but found {len(csv_files)}\""
      ],
      "id": "73de8fd7",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6 CSV files:\n",
            "/content/data/raw/movies.csv\n",
            "/content/data/raw/recommendation_logs.csv\n",
            "/content/data/raw/reviews.csv\n",
            "/content/data/raw/search_logs.csv\n",
            "/content/data/raw/users.csv\n",
            "/content/data/raw/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sehylql0Xofm"
      },
      "source": [
        "**Reflection:** Why is keeping a clean file inventory (names, sizes) useful downstream?"
      ],
      "id": "Sehylql0Xofm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba0n2RyGXofm"
      },
      "source": [
        "## 3) Create GCS bucket & upload — What & Why\n",
        "Stage in GCS → consistent, versionable source for BigQuery loads. Bucket names must be **globally unique**."
      ],
      "id": "Ba0n2RyGXofm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PJBphqfXofm"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates a unique bucket in `${REGION}` (random suffix),\n",
        "- Saves name to `BUCKET_NAME` env var,\n",
        "- Uploads all CSVs to `gs://$BUCKET_NAME/netflix/`,\n",
        "- Prints the bucket name and explains staging benefits.\n"
      ],
      "id": "_PJBphqfXofm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e42d48f",
        "outputId": "a3555ca4-a577-455f-caaf-970c935843f1"
      },
      "source": [
        "import uuid, os\n",
        "# Create a unique bucket name\n",
        "bucket_name = f\"mgmt467-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "\n",
        "# Create the GCS bucket\n",
        "# Buckets are globally unique and reside in a specific region.\n",
        "!gcloud storage buckets create gs://$BUCKET_NAME --location=$REGION\n",
        "\n",
        "# Upload all CSV files to the bucket\n",
        "# Staging data in GCS provides a stable, versionable source for loading into BigQuery.\n",
        "# This is more reliable than loading directly from a temporary Colab environment.\n",
        "!gcloud storage cp /content/data/raw/*.csv gs://$BUCKET_NAME/netflix/\n",
        "\n",
        "# Print the bucket name\n",
        "print(\"Created and uploaded files to GCS bucket:\", bucket_name)\n",
        "\n",
        "# Verify contents\n",
        "!gcloud storage ls gs://$BUCKET_NAME/netflix/"
      ],
      "id": "7e42d48f",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://mgmt467-netflix-22ade7ad/...\n",
            "Copying file:///content/data/raw/movies.csv to gs://mgmt467-netflix-22ade7ad/netflix/movies.csv\n",
            "Copying file:///content/data/raw/recommendation_logs.csv to gs://mgmt467-netflix-22ade7ad/netflix/recommendation_logs.csv\n",
            "Copying file:///content/data/raw/reviews.csv to gs://mgmt467-netflix-22ade7ad/netflix/reviews.csv\n",
            "Copying file:///content/data/raw/search_logs.csv to gs://mgmt467-netflix-22ade7ad/netflix/search_logs.csv\n",
            "Copying file:///content/data/raw/users.csv to gs://mgmt467-netflix-22ade7ad/netflix/users.csv\n",
            "Copying file:///content/data/raw/watch_history.csv to gs://mgmt467-netflix-22ade7ad/netflix/watch_history.csv\n",
            "\n",
            "Average throughput: 23.6MiB/s\n",
            "Created and uploaded files to GCS bucket: mgmt467-netflix-22ade7ad\n",
            "gs://mgmt467-netflix-22ade7ad/netflix/movies.csv\n",
            "gs://mgmt467-netflix-22ade7ad/netflix/recommendation_logs.csv\n",
            "gs://mgmt467-netflix-22ade7ad/netflix/reviews.csv\n",
            "gs://mgmt467-netflix-22ade7ad/netflix/search_logs.csv\n",
            "gs://mgmt467-netflix-22ade7ad/netflix/users.csv\n",
            "gs://mgmt467-netflix-22ade7ad/netflix/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K46mo5l1Xofm"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that lists the `netflix/` prefix and shows object sizes.\n"
      ],
      "id": "K46mo5l1Xofm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0ad960f",
        "outputId": "017add09-af96-4fe9-ffda-5b6d81db376c"
      },
      "source": [
        "# List objects in the bucket with details including size\n",
        "# This verifies the files were uploaded correctly to the specified prefix.\n",
        "!gcloud storage ls -l gs://$BUCKET_NAME/netflix/"
      ],
      "id": "e0ad960f",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    115942  2025-10-26T19:20:55Z  gs://mgmt467-netflix-22ade7ad/netflix/movies.csv\n",
            "   4695557  2025-10-26T19:20:56Z  gs://mgmt467-netflix-22ade7ad/netflix/recommendation_logs.csv\n",
            "   1861942  2025-10-26T19:20:55Z  gs://mgmt467-netflix-22ade7ad/netflix/reviews.csv\n",
            "   2250902  2025-10-26T19:20:56Z  gs://mgmt467-netflix-22ade7ad/netflix/search_logs.csv\n",
            "   1606820  2025-10-26T19:20:55Z  gs://mgmt467-netflix-22ade7ad/netflix/users.csv\n",
            "   9269425  2025-10-26T19:20:56Z  gs://mgmt467-netflix-22ade7ad/netflix/watch_history.csv\n",
            "TOTAL: 6 objects, 19800588 bytes (18.88MiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzjRbBAJXofm"
      },
      "source": [
        "**Reflection:** Name two benefits of staging in GCS vs loading directly from local Colab."
      ],
      "id": "xzjRbBAJXofm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izw7c_4zXofn"
      },
      "source": [
        "## 4) BigQuery dataset & loads — What & Why\n",
        "Create dataset `netflix` and load six CSVs with **autodetect** for speed (we’ll enforce schemas later)."
      ],
      "id": "izw7c_4zXofn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mqbh4D-GXofn"
      },
      "source": [
        "### Build Prompt (two cells)\n",
        "**Cell A:** Create (idempotently) dataset `netflix` in US multi-region; if it exists, print a friendly message.  \n",
        "**Cell B:** Load tables from `gs://$BUCKET_NAME/netflix/`:\n",
        "`users, movies, watch_history, recommendation_logs, search_logs, reviews`\n",
        "with `--skip_leading_rows=1 --autodetect --source_format=CSV`.\n",
        "Finish with row-count queries for each table.\n"
      ],
      "id": "Mqbh4D-GXofn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHT2CxC0Xofn",
        "outputId": "eda32fd9-7038-4842-fbe6-482b5f4b85b8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading users from gs://mgmt467-netflix-22ade7ad/netflix/users.csv\n",
            "Waiting on bqjob_r732d0fb5819d49c5_0000019a21f78eb4_1 ... (1s) Current status: DONE   \n",
            "Loading movies from gs://mgmt467-netflix-22ade7ad/netflix/movies.csv\n",
            "Waiting on bqjob_ra9ffbecbec8faaa_0000019a21f7a5d3_1 ... (1s) Current status: DONE   \n",
            "Loading watch_history from gs://mgmt467-netflix-22ade7ad/netflix/watch_history.csv\n",
            "Waiting on bqjob_r7bfaa7f8555b606a_0000019a21f7bacb_1 ... (2s) Current status: DONE   \n",
            "Loading recommendation_logs from gs://mgmt467-netflix-22ade7ad/netflix/recommendation_logs.csv\n",
            "Waiting on bqjob_r4a2cd0276ada247b_0000019a21f7d514_1 ... (3s) Current status: DONE   \n",
            "Loading search_logs from gs://mgmt467-netflix-22ade7ad/netflix/search_logs.csv\n",
            "Waiting on bqjob_r5d0be98f7d040bd0_0000019a21f7f259_1 ... (1s) Current status: DONE   \n",
            "Loading reviews from gs://mgmt467-netflix-22ade7ad/netflix/reviews.csv\n",
            "Waiting on bqjob_r70c8074c81c577a7_0000019a21f80748_1 ... (1s) Current status: DONE   \n",
            "\n",
            "Verifying row counts:\n",
            "Table: users, Row Count: 103000\n",
            "Table: movies, Row Count: 10400\n",
            "Table: watch_history, Row Count: 1050000\n",
            "Table: recommendation_logs, Row Count: 520000\n",
            "Table: search_logs, Row Count: 265000\n",
            "Table: reviews, Row Count: 154500\n"
          ]
        }
      ],
      "source": [
        "# Load data into BigQuery tables from GCS\n",
        "# `--skip_leading_rows=1` skips the header row.\n",
        "# `--autodetect` allows BigQuery to infer the schema.\n",
        "# `--source_format=CSV` specifies the file format.\n",
        "tables = {\n",
        "  \"users\": \"users.csv\",\n",
        "  \"movies\": \"movies.csv\",\n",
        "  \"watch_history\": \"watch_history.csv\",\n",
        "  \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "  \"search_logs\": \"search_logs.csv\",\n",
        "  \"reviews\": \"reviews.csv\",\n",
        "}\n",
        "import os\n",
        "DATASET=\"netflix\" # Define DATASET variable\n",
        "for tbl, fname in tables.items():\n",
        "  src = f\"gs://{os.environ['BUCKET_NAME']}/netflix/{fname}\"\n",
        "  print(f\"Loading {tbl} from {src}\")\n",
        "  !bq load --skip_leading_rows=1 --autodetect --source_format=CSV {DATASET}.{tbl} {src}\n",
        "\n",
        "# Row counts for verification\n",
        "print(\"\\nVerifying row counts:\")\n",
        "from google.cloud import bigquery\n",
        "client = bigquery.Client(project=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "for tbl in tables.keys():\n",
        "  query = f\"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.{tbl}`\"\n",
        "  query_job = client.query(query)\n",
        "  results = query_job.result()\n",
        "  for row in results:\n",
        "    print(f\"Table: {row.table_name}, Row Count: {row.n}\")"
      ],
      "id": "KHT2CxC0Xofn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj-4TH4GXofn"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single query that returns `table_name, row_count` for all six tables in `${GOOGLE_CLOUD_PROJECT}.netflix`.\n"
      ],
      "id": "fj-4TH4GXofn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6864aca",
        "outputId": "1fec1d6d-f772-45cf-91a1-4e5d365bb1ed"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT 'users' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.users`\n",
        "UNION ALL\n",
        "SELECT 'movies' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.movies`\n",
        "UNION ALL\n",
        "SELECT 'watch_history' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.watch_history`\n",
        "UNION ALL\n",
        "SELECT 'recommendation_logs' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.recommendation_logs`\n",
        "UNION ALL\n",
        "SELECT 'search_logs' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.search_logs`\n",
        "UNION ALL\n",
        "SELECT 'reviews' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.reviews`\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "print(\"Row counts for tables in netflix dataset:\")\n",
        "for row in results:\n",
        "    print(f\"Table: {row.table_name}, Row Count: {row.row_count}\")"
      ],
      "id": "e6864aca",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row counts for tables in netflix dataset:\n",
            "Table: reviews, Row Count: 154500\n",
            "Table: recommendation_logs, Row Count: 520000\n",
            "Table: search_logs, Row Count: 265000\n",
            "Table: movies, Row Count: 10400\n",
            "Table: watch_history, Row Count: 1050000\n",
            "Table: users, Row Count: 103000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTe7Bj3sXofn"
      },
      "source": [
        "**Reflection:** When is `autodetect` acceptable? When should you enforce explicit schemas and why?"
      ],
      "id": "VTe7Bj3sXofn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_3QYLnkXofn"
      },
      "source": [
        "## 5) Data Quality (DQ) — Concepts we care about\n",
        "- **Missingness** (MCAR/MAR/MNAR). Impute vs drop. Add `is_missing_*` indicators.\n",
        "- **Duplicates** (exact vs near). Double-counted engagement corrupts labels & KPIs.\n",
        "- **Outliers** (IQR). Winsorize/cap vs robust models. Always **flag** and explain.\n",
        "- **Reproducibility**. Prefer `CREATE OR REPLACE` and deterministic keys.\n"
      ],
      "id": "L_3QYLnkXofn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRlYTzZAXofn"
      },
      "source": [
        "### 5.1 Missingness (users) — What & Why\n",
        "Measure % missing and check if missingness depends on another variable (MAR) → potential bias & instability."
      ],
      "id": "oRlYTzZAXofn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v31jtZpMXofn"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Total rows and % missing in `region`, `plan_tier`, `age_band` from `users`.\n",
        "2) `% plan_tier missing by region` ordered descending. Add comments on MAR.\n"
      ],
      "id": "v31jtZpMXofn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "834466f4",
        "outputId": "c38cce69-6cfa-4663-df8c-f54dbe081467"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Query 1: Total rows and % missing per column\n",
        "query_missingness_profile = f\"\"\"\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(country IS NULL) miss_region,\n",
        "         COUNTIF(subscription_plan IS NULL) miss_plan,\n",
        "         COUNTIF(age IS NULL) miss_age\n",
        "  FROM `{project_id}.netflix.users`\n",
        ")\n",
        "SELECT n,\n",
        "       ROUND(100*miss_region/n,2) AS pct_missing_region,\n",
        "       ROUND(100*miss_plan/n,2)   AS pct_missing_plan_tier,\n",
        "       ROUND(100*miss_age/n,2)    AS pct_missing_age_band\n",
        "FROM base;\n",
        "\"\"\"\n",
        "\n",
        "print(\"Missingness Profile (users table):\")\n",
        "query_job_profile = client.query(query_missingness_profile)\n",
        "results_profile = query_job_profile.result()\n",
        "\n",
        "for row in results_profile:\n",
        "    print(f\"Total Rows: {row.n}\")\n",
        "    print(f\"Pct Missing Region: {row.pct_missing_region}%\")\n",
        "    print(f\"Pct Missing Plan Tier: {row.pct_missing_plan_tier}%\")\n",
        "    print(f\"Pct Missing Age Band: {row.pct_missing_age_band}%\")"
      ],
      "id": "834466f4",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missingness Profile (users table):\n",
            "Total Rows: 103000\n",
            "Pct Missing Region: 0.0%\n",
            "Pct Missing Plan Tier: 0.0%\n",
            "Pct Missing Age Band: 11.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a40ebd33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61452dbf-10d9-499c-b172-12c74498ea05"
      },
      "source": [
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Query 2: % plan_tier missing by region (checking for MAR)\n",
        "query_mar_by_region = f\"\"\"\n",
        "-- % plan_tier missing by region ordered descending (checking for MAR - Missing At Random)\n",
        "-- This query helps identify if the missingness of 'plan_tier' is dependent on the 'region'.\n",
        "SELECT country,\n",
        "       COUNT(*) AS n,\n",
        "       ROUND(100*COUNTIF(subscription_plan IS NULL)/COUNT(*),2) AS pct_missing_plan_tier\n",
        "FROM `{project_id}.netflix.users`\n",
        "GROUP BY country\n",
        "ORDER BY pct_missing_plan_tier DESC;\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n% Plan Tier Missing by Region:\")\n",
        "query_job_mar = client.query(query_mar_by_region)\n",
        "results_mar = query_job_mar.result()\n",
        "\n",
        "for row in results_mar:\n",
        "    print(f\"Region: {row.country}, Total Rows: {row.n}, Pct Missing Plan Tier: {row.pct_missing_plan_tier}%\")"
      ],
      "id": "a40ebd33",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "% Plan Tier Missing by Region:\n",
            "Region: Canada, Total Rows: 30960, Pct Missing Plan Tier: 0.0%\n",
            "Region: USA, Total Rows: 72040, Pct Missing Plan Tier: 0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORizZ115Xofo"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that prints the three missingness percentages from (1), rounded to two decimals.\n"
      ],
      "id": "ORizZ115Xofo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d117e9e5",
        "outputId": "6b213ec0-7846-411b-d644-423988144e9e"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Query to print the three missingness percentages\n",
        "query_missingness_summary = f\"\"\"\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(country IS NULL) miss_region,\n",
        "         COUNTIF(subscription_plan IS NULL) miss_plan,\n",
        "         COUNTIF(age IS NULL) miss_age\n",
        "  FROM `{project_id}.netflix.users`\n",
        ")\n",
        "SELECT\n",
        "       ROUND(100*miss_region/n,2) AS pct_missing_region,\n",
        "       ROUND(100*miss_plan/n,2)   AS pct_missing_plan_tier,\n",
        "       ROUND(100*miss_age/n,2)    AS pct_missing_age_band\n",
        "FROM base;\n",
        "\"\"\"\n",
        "\n",
        "print(\"Missingness Percentages:\")\n",
        "query_job_summary = client.query(query_missingness_summary)\n",
        "results_summary = query_job_summary.result()\n",
        "\n",
        "for row in results_summary:\n",
        "    print(f\"Pct Missing Region: {row.pct_missing_region}%\")\n",
        "    print(f\"Pct Missing Plan Tier: {row.pct_missing_plan_tier}%\")\n",
        "    print(f\"Pct Missing Age Band: {row.pct_missing_age_band}%\")"
      ],
      "id": "d117e9e5",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missingness Percentages:\n",
            "Pct Missing Region: 0.0%\n",
            "Pct Missing Plan Tier: 0.0%\n",
            "Pct Missing Age Band: 11.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk_32L2xXofo"
      },
      "source": [
        "**Reflection:** Which columns are most missing? Hypothesize MCAR/MAR/MNAR and why."
      ],
      "id": "Jk_32L2xXofo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aid5Au_fXofo"
      },
      "source": [
        "### 5.2 Duplicates (watch_history) — What & Why\n",
        "Find exact duplicate interaction records and keep **one best** per group (deterministic policy)."
      ],
      "id": "Aid5Au_fXofo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7GOOPpAXofo"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Report duplicate groups on `(user_id, movie_id, event_ts, device_type)` with counts (top 20).\n",
        "2) Create table `watch_history_dedup` that keeps one row per group (prefer higher `progress_ratio`, then `minutes_watched`). Add comments.\n"
      ],
      "id": "X7GOOPpAXofo"
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_grp_query = f\"\"\"-- Find duplicate groups in Netflix watch history\n",
        "-- Grouping by (user_id, movie_id, event_ts, device_type)\n",
        "-- Count how many rows exist per group, then filter to only duplicates\n",
        "-- Show the top 20 groups with the highest duplicate counts\n",
        "\n",
        "SELECT\n",
        "  user_id,\n",
        "  movie_id,\n",
        "  watch_date,\n",
        "  device_type,\n",
        "  COUNT(*) AS duplicate_count\n",
        "FROM\n",
        "  `{project_id}.netflix.watch_history`\n",
        "GROUP BY\n",
        "  user_id, movie_id, watch_date, device_type\n",
        "HAVING\n",
        "  COUNT(*) > 1\n",
        "ORDER BY\n",
        "  duplicate_count DESC\n",
        "LIMIT 20;\n",
        "\"\"\"\n",
        "\n",
        "duplicate_grp_query_job = client.query(duplicate_grp_query)\n",
        "duplicate_grp_results = duplicate_grp_query_job.result()\n",
        "for row in duplicate_grp_results:\n",
        "    print(f\"User ID: {row.user_id}\")\n",
        "    print(f\"Movie ID: {row.movie_id}\")\n",
        "    print(f\"Watch Date: {row.watch_date}\")\n",
        "    print(f\"Device Type: {row.device_type}\")\n",
        "    print(f\"Duplicate Count: {row.duplicate_count}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqs6MMoj_cGl",
        "outputId": "78bfc2e3-a110-4e64-d5d6-7d3c6b3ea93e"
      },
      "id": "sqs6MMoj_cGl",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User ID: user_03310\n",
            "Movie ID: movie_0640\n",
            "Watch Date: 2024-09-08\n",
            "Device Type: Smart TV\n",
            "Duplicate Count: 40\n",
            "\n",
            "\n",
            "User ID: user_00391\n",
            "Movie ID: movie_0893\n",
            "Watch Date: 2024-08-26\n",
            "Device Type: Laptop\n",
            "Duplicate Count: 40\n",
            "\n",
            "\n",
            "User ID: user_03043\n",
            "Movie ID: movie_0465\n",
            "Watch Date: 2024-02-03\n",
            "Device Type: Laptop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_04899\n",
            "Movie ID: movie_0142\n",
            "Watch Date: 2025-01-20\n",
            "Device Type: Desktop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_01143\n",
            "Movie ID: movie_0166\n",
            "Watch Date: 2024-05-28\n",
            "Device Type: Laptop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_06799\n",
            "Movie ID: movie_0458\n",
            "Watch Date: 2024-08-15\n",
            "Device Type: Desktop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_09564\n",
            "Movie ID: movie_0552\n",
            "Watch Date: 2025-01-11\n",
            "Device Type: Laptop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_02126\n",
            "Movie ID: movie_0642\n",
            "Watch Date: 2025-02-09\n",
            "Device Type: Desktop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_03348\n",
            "Movie ID: movie_0688\n",
            "Watch Date: 2024-01-22\n",
            "Device Type: Desktop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_00564\n",
            "Movie ID: movie_0234\n",
            "Watch Date: 2024-01-09\n",
            "Device Type: Laptop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_06535\n",
            "Movie ID: movie_0890\n",
            "Watch Date: 2025-05-23\n",
            "Device Type: Desktop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_09973\n",
            "Movie ID: movie_0342\n",
            "Watch Date: 2025-03-22\n",
            "Device Type: Desktop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_02652\n",
            "Movie ID: movie_0352\n",
            "Watch Date: 2024-10-22\n",
            "Device Type: Desktop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_00928\n",
            "Movie ID: movie_0913\n",
            "Watch Date: 2024-01-18\n",
            "Device Type: Laptop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_06295\n",
            "Movie ID: movie_0097\n",
            "Watch Date: 2025-02-24\n",
            "Device Type: Desktop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_04506\n",
            "Movie ID: movie_0244\n",
            "Watch Date: 2025-05-27\n",
            "Device Type: Desktop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_03408\n",
            "Movie ID: movie_0146\n",
            "Watch Date: 2025-06-02\n",
            "Device Type: Desktop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_09815\n",
            "Movie ID: movie_0827\n",
            "Watch Date: 2024-05-25\n",
            "Device Type: Laptop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_01469\n",
            "Movie ID: movie_0237\n",
            "Watch Date: 2025-01-17\n",
            "Device Type: Laptop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n",
            "User ID: user_02284\n",
            "Movie ID: movie_0914\n",
            "Watch Date: 2024-12-30\n",
            "Device Type: Laptop\n",
            "Duplicate Count: 30\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dedup_query = f\"\"\"\n",
        "-- Create a deduplicated version of watch_history\n",
        "-- Keep only one row per (user_id, movie_id, event_ts, device_type)\n",
        "-- Preference order:\n",
        "--   1. Higher progress_ratio\n",
        "--   2. If tie, higher minutes_watched\n",
        "-- Use QUALIFY with ROW_NUMBER for deterministic selection\n",
        "\n",
        "CREATE OR REPLACE TABLE `{project_id}.netflix.watch_history_dedup` AS\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  `{project_id}.netflix.watch_history`\n",
        "QUALIFY\n",
        "  ROW_NUMBER() OVER (\n",
        "    PARTITION BY user_id, movie_id, watch_date, device_type\n",
        "    ORDER BY progress_percentage DESC, watch_duration_minutes DESC\n",
        "  ) = 1;\n",
        "\n",
        "\"\"\"\n",
        "dedup_query_job = client.query(dedup_query)\n",
        "dedup_results = dedup_query_job.result()\n",
        "for row in dedup_results:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "GoBHu9cMBbd7"
      },
      "id": "GoBHu9cMBbd7",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7LHZT5yXof0"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a before/after count query comparing raw vs `watch_history_dedup`.\n"
      ],
      "id": "F7LHZT5yXof0"
    },
    {
      "cell_type": "code",
      "source": [
        "dedup_verification_query = f\"\"\"\n",
        "-- Validation query: compare row counts before and after deduplication\n",
        "-- Shows how many rows existed originally vs. in the deduplicated table\n",
        "-- Also calculates how many rows were removed\n",
        "\n",
        "WITH before_count AS (\n",
        "  SELECT COUNT(*) AS total_rows\n",
        "  FROM {project_id}.netflix.`watch_history`\n",
        "),\n",
        "after_count AS (\n",
        "  SELECT COUNT(*) AS total_rows\n",
        "  FROM {project_id}.netflix.`watch_history_dedup`\n",
        ")\n",
        "SELECT\n",
        "  b.total_rows AS before_count,\n",
        "  a.total_rows AS after_count,\n",
        "  b.total_rows - a.total_rows AS rows_removed\n",
        "FROM before_count b\n",
        "CROSS JOIN after_count a;\n",
        "\"\"\"\n",
        "\n",
        "dedup_verification_query_job = client.query(dedup_verification_query)\n",
        "dedup_verification_results = dedup_verification_query_job.result()\n",
        "for row in dedup_verification_results:\n",
        "    print(f\"Before Count: {row.before_count}\")\n",
        "    print(f\"After Count: {row.after_count}\")\n",
        "    print(f\"Rows Removed: {row.rows_removed}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrYb2TkvDYv0",
        "outputId": "64f6098a-0ff6-41b7-a5c3-57b6e4499d84"
      },
      "id": "DrYb2TkvDYv0",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Count: 1050000\n",
            "After Count: 100000\n",
            "Rows Removed: 950000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRIAQM3oXof0"
      },
      "source": [
        "#### **Reflection:** Why do duplicates arise (natural vs system-generated)? How do they corrupt labels and KPIs?\n",
        "\n",
        "Duplicates can arise from various sources, both natural and system-generated.\n",
        "\n",
        "**Natural duplicates** might occur from user behavior, like accidentally submitting a form twice, or from real-world events being recorded multiple times through different channels.\n",
        "\n",
        "**System-generated duplicates** are often due to errors in data pipelines, ETL processes, or database merges where records are not properly identified and consolidated. Issues like retries on failed API calls or faulty data ingestion scripts can also create duplicates.\n",
        "\n",
        "Duplicates can significantly corrupt labels and KPIs:\n",
        "\n",
        "- **Labels:** If you're building a model to predict user engagement (e.g., watch time), duplicate watch history entries will artificially inflate the engagement metric, leading to inaccurate labels and a biased model.\n",
        "- **KPIs:** Business metrics like \"total minutes watched\" or \"number of active users\" will be inflated by duplicates, giving a false picture of user behavior and business performance. This can lead to poor decision-making based on inaccurate data.\n",
        "\n",
        "Deduplication is a crucial step to ensure the integrity of your data for analysis and modeling."
      ],
      "id": "sRIAQM3oXof0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRI4xBtgXof0"
      },
      "source": [
        "### 5.3 Outliers (minutes_watched) — What & Why\n",
        "Estimate extreme values via IQR; report % outliers; **winsorize** to P01/P99 for robustness while also **flagging** extremes."
      ],
      "id": "sRI4xBtgXof0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKYyej50Xof0"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Compute IQR bounds for `minutes_watched` on `watch_history_dedup` and report % outliers.\n",
        "2) Create `watch_history_robust` with `minutes_watched_capped` capped at P01/P99; return quantile summaries before/after.\n"
      ],
      "id": "uKYyej50Xof0"
    },
    {
      "cell_type": "code",
      "source": [
        "iqr_query = f\"\"\"\n",
        "-- Compute Q1, Q3, and IQR for minutes_watched\n",
        "-- Derive lower/upper bounds using Tukey's rule (Q1 - 1.5*IQR, Q3 + 1.5*IQR)\n",
        "-- Report the percentage of rows outside these bounds as outliers\n",
        "\n",
        "WITH stats AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(25)] AS q1,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(75)] AS q3,\n",
        "    COUNT(*) AS total_rows\n",
        "  FROM {project_id}.netflix.`watch_history_dedup`\n",
        "),\n",
        "bounds AS (\n",
        "  SELECT\n",
        "    q1,\n",
        "    q3,\n",
        "    q3 - q1 AS iqr,\n",
        "    q1 - 1.5 * (q3 - q1) AS lower_bound,\n",
        "    q3 + 1.5 * (q3 - q1) AS upper_bound,\n",
        "    total_rows\n",
        "  FROM stats\n",
        "),\n",
        "outliers AS (\n",
        "  SELECT\n",
        "    COUNTIF(watch_duration_minutes < lower_bound OR watch_duration_minutes > upper_bound) AS outlier_count\n",
        "  FROM {project_id}.netflix.`watch_history_dedup`, bounds\n",
        ")\n",
        "SELECT\n",
        "  b.q1,\n",
        "  b.q3,\n",
        "  b.iqr,\n",
        "  b.lower_bound,\n",
        "  b.upper_bound,\n",
        "  o.outlier_count,\n",
        "  b.total_rows,\n",
        "  SAFE_DIVIDE(o.outlier_count, b.total_rows) * 100 AS pct_outliers\n",
        "FROM bounds b\n",
        "CROSS JOIN outliers o;\n",
        "\"\"\"\n",
        "iqr_query_job = client.query(iqr_query)\n",
        "iqr_results = iqr_query_job.result()\n",
        "for row in iqr_results:\n",
        "    print(f\"Q1: {row.q1}\")\n",
        "    print(f\"Q3: {row.q3}\")\n",
        "    print(f\"IQR: {row.iqr}\")\n",
        "    print(f\"Lower Bound: {row.lower_bound}\")\n",
        "    print(f\"Upper Bound: {row.upper_bound}\")\n",
        "    print(f\"Outlier Count: {row.outlier_count}\")\n",
        "    print(f\"Total Rows: {row.total_rows}\")\n",
        "    print(f\"Percentage Outliers: {row.pct_outliers}%\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY9YBQSNhg9S",
        "outputId": "0d24b8ba-ab1d-4a7c-9837-7fae30fb1175"
      },
      "id": "nY9YBQSNhg9S",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1: 29.1\n",
            "Q3: 82.4\n",
            "IQR: 53.300000000000004\n",
            "Lower Bound: -50.85\n",
            "Upper Bound: 162.35000000000002\n",
            "Outlier Count: 3531\n",
            "Total Rows: 100000\n",
            "Percentage Outliers: 3.531%\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantile_query = f\"\"\"\n",
        "-- Step 1: Compute P01 and P99 for minutes_watched\n",
        "-- Step 2: Create watch_history_robust with capped values\n",
        "-- Step 3: Return quantile summaries before/after capping for comparison\n",
        "\n",
        "-- Create the robust table with capped minutes_watched\n",
        "CREATE OR REPLACE TABLE {project_id}.netflix.`watch_history_robust` AS\n",
        "WITH percentiles AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(1)] AS p01,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(99)] AS p99\n",
        "  FROM {project_id}.netflix.`watch_history_dedup`\n",
        ")\n",
        "SELECT\n",
        "  wh.*,\n",
        "  LEAST(GREATEST(watch_duration_minutes, p01), p99) AS minutes_watched_capped\n",
        "FROM {project_id}.netflix.`watch_history_dedup` wh\n",
        "CROSS JOIN percentiles;\n",
        "\n",
        "-- Summaries before/after capping (deciles shown for readability)\n",
        "WITH before AS (\n",
        "  SELECT 'before' AS stage,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 10) AS deciles\n",
        "  FROM {project_id}.netflix.`watch_history_dedup`\n",
        "),\n",
        "after AS (\n",
        "  SELECT 'after' AS stage,\n",
        "    APPROX_QUANTILES(minutes_watched_capped, 10) AS deciles\n",
        "  FROM {project_id}.netflix.`watch_history_robust`\n",
        ")\n",
        "SELECT * FROM before\n",
        "UNION ALL\n",
        "SELECT * FROM after;\n",
        "\"\"\"\n",
        "quantile_query_job = client.query(quantile_query)\n",
        "quantile_results = quantile_query_job.result()\n",
        "for row in quantile_results:\n",
        "    print(f\"Stage: {row.stage}\")\n",
        "    print(f\"Deciles: {row.deciles}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJz0dEFJh0os",
        "outputId": "b8a3c6b6-ce66-4d87-a61e-300e49d7e50e"
      },
      "id": "oJz0dEFJh0os",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage: after\n",
            "Deciles: [4.4, 16.1, 24.9, 33.2, 41.8, 51.3, 61.9, 74.6, 91.6, 120.7, 366.0]\n",
            "\n",
            "\n",
            "Stage: before\n",
            "Deciles: [0.2, 16.1, 24.9, 33.1, 41.9, 51.1, 61.5, 74.7, 91.8, 121.1, 799.3]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObOsbl72Xof1"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that shows min/median/max before vs after capping.\n"
      ],
      "id": "ObOsbl72Xof1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nGQZsw0Xof1"
      },
      "source": [
        "**Reflection:** When might capping be harmful? Name a model type less sensitive to outliers and why."
      ],
      "id": "5nGQZsw0Xof1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4T0jsMPXof1"
      },
      "source": [
        "### 5.4 Business anomaly flags — What & Why\n",
        "Human-readable flags help both product decisioning and ML features (e.g., binge behavior)."
      ],
      "id": "k4T0jsMPXof1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgOvd6T9Xof1"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **three BigQuery SQL cells** (adjust if columns differ):\n",
        "1) In `watch_history_robust`, compute and summarize `flag_binge` for sessions > 8 hours.\n",
        "2) In `users`, compute and summarize `flag_age_extreme` if age can be parsed from `age_band` (<10 or >100).\n",
        "3) In `movies`, compute and summarize `flag_duration_anomaly` where `duration_min` < 15 or > 480 (if exists).\n",
        "Each cell should output count and percentage and include 1–2 comments.\n"
      ],
      "id": "RgOvd6T9Xof1"
    },
    {
      "cell_type": "code",
      "source": [
        "binge_query = f\"\"\"\n",
        "-- Flag sessions where minutes_watched exceeds 8 hours (480 minutes)\n",
        "-- Summarize count and percentage of flagged binge sessions\n",
        "\n",
        "WITH base AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    minutes_watched_capped > 480 AS flag_binge\n",
        "  FROM {project_id}.netflix.`watch_history_robust`\n",
        ")\n",
        "SELECT\n",
        "  COUNTIF(flag_binge) AS binge_count,\n",
        "  COUNT(*) AS total_rows,\n",
        "  SAFE_DIVIDE(COUNTIF(flag_binge), COUNT(*)) * 100 AS pct_binge\n",
        "FROM base;\n",
        "\"\"\"\n",
        "binge_query_job = client.query(binge_query)\n",
        "binge_results = binge_query_job.result()\n",
        "for row in binge_results:\n",
        "    print(f\"Binge Count: {row.binge_count}\")\n",
        "    print(f\"Total Rows: {row.total_rows}\")\n",
        "    print(f\"Percentage Binge: {row.pct_binge}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdFvOyamic36",
        "outputId": "93977c9b-9c71-48fa-bf84-a0322c9a8f7b"
      },
      "id": "GdFvOyamic36",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binge Count: 0\n",
            "Total Rows: 100000\n",
            "Percentage Binge: 0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extreme_age_query = f\"\"\"\n",
        "-- Flag extreme ages in users table\n",
        "-- Extreme defined as age < 10 or age > 100\n",
        "-- Summarize count and percentage of flagged rows\n",
        "\n",
        "WITH flagged AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    (age < 10 OR age > 100) AS flag_age_extreme\n",
        "  FROM {project_id}.netflix.`users`\n",
        ")\n",
        "SELECT\n",
        "  COUNTIF(flag_age_extreme) AS extreme_age_count,\n",
        "  COUNT(*) AS total_rows,\n",
        "  SAFE_DIVIDE(COUNTIF(flag_age_extreme), COUNT(*)) * 100 AS pct_extreme_age\n",
        "FROM flagged;\n",
        "\"\"\"\n",
        "extreme_age_query_job = client.query(extreme_age_query)\n",
        "extreme_age_results = extreme_age_query_job.result()\n",
        "for row in extreme_age_results:\n",
        "    print(f\"Extreme Age Count: {row.extreme_age_count}\")\n",
        "    print(f\"Total Rows: {row.total_rows}\")\n",
        "    print(f\"Percentage Extreme Age: {row.pct_extreme_age}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ672k2vk0uI",
        "outputId": "1c78ec14-57d0-462c-fdf7-24fbeb8a6103"
      },
      "id": "IQ672k2vk0uI",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extreme Age Count: 1790\n",
            "Total Rows: 103000\n",
            "Percentage Extreme Age: 1.7378640776699028%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "duration_query = f\"\"\"\n",
        "-- Flag movies with implausible durations (<15 minutes or >480 minutes)\n",
        "-- Summarize count and percentage of anomalies\n",
        "\n",
        "WITH flagged AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    (duration_minutes < 15 OR duration_minutes > 480) AS flag_duration_anomaly\n",
        "  FROM {project_id}.netflix.`movies`\n",
        ")\n",
        "SELECT\n",
        "  COUNTIF(flag_duration_anomaly) AS anomaly_count,\n",
        "  COUNT(*) AS total_rows,\n",
        "  SAFE_DIVIDE(COUNTIF(flag_duration_anomaly), COUNT(*)) * 100 AS pct_anomaly\n",
        "FROM flagged;\n",
        "\"\"\"\n",
        "duration_query_job = client.query(duration_query)\n",
        "duration_results = duration_query_job.result()\n",
        "for row in duration_results:\n",
        "    print(f\"Anomaly Count: {row.anomaly_count}\")\n",
        "    print(f\"Total Rows: {row.total_rows}\")\n",
        "    print(f\"Percentage Anomaly: {row.pct_anomaly}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtUufPCXlcy7",
        "outputId": "7a07d825-876c-4111-ae81-07f3c58e08ee"
      },
      "id": "wtUufPCXlcy7",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anomaly Count: 230\n",
            "Total Rows: 10400\n",
            "Percentage Anomaly: 2.2115384615384617%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCHtn0YQXof1"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single compact summary query that returns two columns per flag: `flag_name, pct_of_rows`.\n"
      ],
      "id": "qCHtn0YQXof1"
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_query = f\"\"\"\n",
        "-- Compact anomaly summary: returns flag_name and pct_of_rows for each flag\n",
        "\n",
        "WITH binge AS (\n",
        "  SELECT\n",
        "    'flag_binge' AS flag_name,\n",
        "    SAFE_DIVIDE(COUNTIF(minutes_watched_capped > 480), COUNT(*)) * 100 AS pct_of_rows\n",
        "  FROM {project_id}.netflix.`watch_history_robust`\n",
        "),\n",
        "age_extreme AS (\n",
        "  SELECT\n",
        "    'flag_age_extreme' AS flag_name,\n",
        "    SAFE_DIVIDE(\n",
        "      COUNTIF(age < 10 OR age > 100),\n",
        "      COUNT(*)\n",
        "    ) * 100 AS pct_of_rows\n",
        "  FROM {project_id}.netflix.`users`\n",
        "),\n",
        "duration_anomaly AS (\n",
        "  SELECT\n",
        "    'flag_duration_anomaly' AS flag_name,\n",
        "    SAFE_DIVIDE(COUNTIF(duration_minutes < 15 OR duration_minutes > 480), COUNT(*)) * 100 AS pct_of_rows\n",
        "  FROM {project_id}.netflix.`movies`\n",
        ")\n",
        "SELECT * FROM binge\n",
        "UNION ALL\n",
        "SELECT * FROM age_extreme\n",
        "UNION ALL\n",
        "SELECT * FROM duration_anomaly;\n",
        "\"\"\"\n",
        "anomaly_query_job = client.query(anomaly_query)\n",
        "anomaly_results = anomaly_query_job.result()\n",
        "for row in anomaly_results:\n",
        "    print(f\"Flag Name: {row.flag_name}\")\n",
        "    print(f\"Percentage of Rows Flagged: {row.pct_of_rows}%\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc2-AccQ9M6I",
        "outputId": "e7dc61f3-2d92-44c6-ec45-96721b176b74"
      },
      "id": "zc2-AccQ9M6I",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flag Name: flag_binge\n",
            "Percentage of Rows Flagged: 0.0%\n",
            "\n",
            "\n",
            "Flag Name: flag_age_extreme\n",
            "Percentage of Rows Flagged: 1.7378640776699028%\n",
            "\n",
            "\n",
            "Flag Name: flag_duration_anomaly\n",
            "Percentage of Rows Flagged: 2.2115384615384617%\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1F6PZS8Xof1"
      },
      "source": [
        "**Reflection:** Which anomaly flag is most common? Which would you keep as a feature and why?"
      ],
      "id": "N1F6PZS8Xof1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO7SYHahXof1"
      },
      "source": [
        "## 6) Save & submit — What & Why\n",
        "Reproducibility: save artifacts and document decisions so others can rerun and audit."
      ],
      "id": "zO7SYHahXof1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsaNCdMKXof1"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a checklist (Markdown) students can paste at the end:\n",
        "- Save this notebook to the team Drive.\n",
        "- Export a `.sql` file with your DQ queries and save to repo.\n",
        "- Push notebook + SQL to the **team GitHub** with a descriptive commit.\n",
        "- Add a README with your `PROJECT_ID`, `REGION`, bucket, dataset, and today’s row counts.\n"
      ],
      "id": "JsaNCdMKXof1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_-dSmqOXof1"
      },
      "source": [
        "## Grading rubric (quick)\n",
        "- Profiling completeness (30)  \n",
        "- Cleaning policy correctness & reproducibility (40)  \n",
        "- Reflection/insight (20)  \n",
        "- Hygiene (naming, verification, idempotence) (10)\n"
      ],
      "id": "k_-dSmqOXof1"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}